'''
Created on 2017年7月21日

@author: moonlit
'''
import jieba
import multiprocessing  
import codecs  
from gensim.models.word2vec import Word2Vec

train = True

class WikiData:  
    def __init__(self, wiki_file ):
        self.wiki_file = wiki_file

    def __iter__(self):
        for line in self.wiki_file:
            s = jieba.cut(line)
            #使用filter过滤掉空格和换行，可以考虑同时过滤掉英文单词
            s = filter(lambda x: x != ' ' and x != '\n', s)
            yield list(s)
            
data = "D:\Competition\src\data\zhwiki.txt"
save = "D:\Competition\src\data\zhwiki.save"

model = Word2Vec.load(save)  
print("load:" + save)

if train :
    wiki_data = WikiData(codecs.open(data, 'r', encoding='utf-8'))  
    model = Word2Vec(wiki_data, size=300, window=5, min_count=1,  
                   workers=multiprocessing.cpu_count())    
    model.save(save)              
  
# if not train :  
#     model = Word2Vec.load(save)     
    
print(model.most_similar([u'习近平']))  
print(model.most_similar([u'数据']) )