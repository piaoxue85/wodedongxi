'''
Created on 2017年7月18日

@author: moonlit
'''
import json
import pandas as pd
import numpy  as np
import jieba

import matplotlib.pyplot as plt
from sklearn.preprocessing import scale
from sklearn.utils import shuffle  
from keras.layers import Input, Dense, LSTM, merge,Flatten
from keras.layers.convolutional import Conv1D,Conv2D
from keras.models import Model
from keras.layers import Embedding 
import keras
from sklearn.utils import shuffle  

from gensim.models.word2vec import Word2Vec
# 基础参数配置
class conf:
    #设置用于训练和回测的开始/结束日期
    start_date = '2014-01-01'
    split_date = '2017-01-01'
    end_date = '2017-07-05'
    fields = ['close','amount', 'market_cap','open', 'high', 'low',  'volume']  # features
    seq_len = 768479 #每个input的长度
    batch = 100 #整数，指定进行梯度下降时每个batch包含的样本数,训练时一个batch的样本会被计算一次梯度下降，使目标函数优化一步               
    
    savedFile = "saved.h5"
    train = True
    
    


# def getdata():
#     file = open("D:/Competition/src/data/CodeAndAnnouncements_wordVec.json",'r', encoding='utf_8_sig')
#     data = json.load(file)
#     
#     x = []
#     y = []
#     
#     for info in data :
#         y1 = (info["d1_open"]-info["d0_open"])/info["d0_open"]
#         y2 = (info["d2_open"]-info["d1_open"])/info["d1_open"]
#         y3 = (info["d3_open"]-info["d2_open"])/info["d2_open"]
#         
#         newses = eval(info["Announcements"] )
#         
#         title        = []
#         annonce_type = []
#         content      = []
#         x_           = []
#         
#         for news in newses :
#     #         print(news.keys())
#             title        += news["title"]
#             annonce_type += news["annonce_type"]
#             content      += news["content"]
#             
# #             if len(content) <11 :
# #                 print(news["news_id"])
#             
#         
#         x_ += content+annonce_type+title
#         
#         if len(x_ ) > 11 :
#             #768479
#             if len(x_ ) < 20 :
#                 print(x_)
#             
#             if 768479-len(x_)>0 :
#                 padding = np.zeros((768479-len(x_)), dtype=np.int)
#             x_ += list(padding)            
#             x.append(x_)
#             y.append([y1,y2,y3]) 
#             
#     df = pd.DataFrame(columns=['x', 'y'])
#     df["x"] = x
#     df["y"] = y     
#     df = shuffle(df)
#     x = list(df["x"].values)
#     y = list(df["y"].values)    
#     return x,y    



# 自定义激活函数
import tensorflow as tf
def atan(x): 
    return tf.atan(x)

def getModel(weights):
    # 构建神经网络层 1层LSTM层+3层Dense层
    # 用于1个输入情况
    lstm_input = Input(shape=(768479,-1 ), name='lstm_input')
    embedding_layer = Embedding(input_dim=weights.shape[0], output_dim=weights.shape[1], weights=[weights],trainable=False)(lstm_input)    
    lstm_output = LSTM(128, activation=atan, dropout_W=0.2, dropout_U=0.1,return_sequences=True)(embedding_layer)
    lstm_output1 = LSTM(64, activation=atan, dropout_W=0.2, dropout_U=0.1,return_sequences=True)(lstm_output)
    lstm_output2 = LSTM(32, activation=atan, dropout_W=0.2, dropout_U=0.1,return_sequences=True)(lstm_output1)
    # lstm_output = LSTM(128, activation='tanh', dropout_W=0.2, dropout_U=0.1,return_sequences=True)(lstm_input)
    cnn_output =Conv1D(8,3)(lstm_output2)
    # cnn_output2 =Conv1D(128,3)(lstm_output)
    flatten = Flatten()(cnn_output)
    Dense_output_1 = Dense(8, activation='linear')(flatten)
    Dense_output_2 = Dense(4, activation='linear')(Dense_output_1)
    # Dense_output_1 = Dense(64, activation='relu')(lstm_output)
    # Dense_output_2 = Dense(16, activation='relu')(Dense_output_1)
    predictions = Dense(3, activation=atan)(Dense_output_2)
    # predictions = Dense(1, activation='tanh')(Dense_output_2)
    
    model = Model(input=embedding_layer, output=predictions)
    return model

def generate(data):  
        x = []
        y = []      
        count = 0  
        while  True:
            for info in data:
                y1 = (info["d1_open"]-info["d0_open"])/info["d0_open"]
                y2 = (info["d2_open"]-info["d1_open"])/info["d1_open"]
                y3 = (info["d3_open"]-info["d2_open"])/info["d2_open"]
                
                newses = eval(info["Announcements"] )
                
                title        = []
                annonce_type = []
                content      = []
                x_           = []
                
                for news in newses :
            #         print(news.keys())
                    title        += news["title"]
                    annonce_type += news["annonce_type"]
                    content      += news["content"]
                    
        #             if len(content) <11 :
        #                 print(news["news_id"])
                    
                
                x_ += content+annonce_type+title
                
                if len(x_ ) > 1000 :
                    #768479
                    if len(x_ ) < 20 :
                        print(x_)
                    
                    if 768479-len(x_)>0 :
                        padding = np.zeros((768479-len(x_)), dtype=np.int)
                        x_ += list(padding)            
    
                x.append(x_)
                y.append([y1,y2,y3])
                count += 1
#                 if count == 100 :
#                     x = np.array(x).reshape(100,768479)
    #                 y = np.array(y ).reshape((100,-1))
                x = np.array(x)
                y = np.array(y)
    #                 print(x.shape)
    #                 print(y.shape)
                    
                count = 0
                yield x,y
                x = []
                y = []
                
                                

file = open("D:/Competition/src/data/CodeAndAnnouncements_wordVec.json",'r', encoding='utf_8_sig')
data = json.load(file)
# generate(data)


#载入保存的文件
modelWord2Vec = Word2Vec.load("D:\Competition\src\data\SogouCA\corpus.model")    
#获得权重
weights = modelWord2Vec.wv.syn0  


model = getModel(weights)
model.compile(optimizer='adam', loss='mse', metrics=['mse','accuracy'])
try :
    model.load_weights(conf.savedFile)
    print("model loaded")
except :    
    print("load model err")
    
if conf.train :
#     early_stopping = EarlyStopping(monitor='loss', patience=2)
    model.fit_generator(generate(data), steps_per_epoch=2000,epochs=50)
    model.save_weights(conf.savedFile)
    print("model saved")
#     score = model.evaluate(test_x, test_y, batch_size = conf.batch)
#     print("%s: %.2f%%" % (model.metrics_names[1], score[1] * 100))
else :
    pass


        
        


 
